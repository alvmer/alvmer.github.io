## Введение 

Нередко при работе со структурами данных мы сталкиваемся с частыми последовательными вызовами операций. Можно проследить, что фактическая временная сложность операции зависит от текущего состояния структуры данных, то есть вызовы в разные моменты времени даже одной и той же операции могут сильно отличаться по времени. Говорить о временной сложности в худшем случае при этом может быть  очень грустно: оценка может получиться слишком грубой в том смысле, что реально такая сложность достигается не так часто. 

Например, вы хотите купить автомобиль за 1 200 000 &#8381;. Предположим, что ваша зарплата составляет 150 000 &#8381;, и вы планируете осуществить покупку за один год. Попробуем оценить сверху ту сумму, которую мы будем перечислять ежемесячно из нашей зарплаты на отдельный счет <<автомобиль мечты>>. Можно не заниматься никакими разделениями средств на своих картах и через одиннадцать месяцев скинуть на счет <<автомобиль мечты>> 1 200 000 &#8381;. В этом случае получается, что наши отчисления в худшем случае составили 1 200 000 &#8381;. Если так оценить вообще все наши ежемесячные отчисления (при том, что большинство из них составили 0 &#8381;), то может показаться, что нашей зарплаты никогда не хватит на покупку автомобиля. Но это не совсем так. Оценка получилась чересчур грубой, хоть и верной.

Можно пойти и по другому пути. Будем отчислять каждый месяц по 100 000 &#8381; на счет <<автомобиль мечты>>. Тогда к последнему месяцу мы наберем необходимую сумму и купим автомобиль, имея зарплату в 150 000 &#8381;. Видно, что оценка 100 000 &#8381;/мес. выглядит куда более реалистичной, так как она уже учитывает в себе тот факт того, что автомобиль покупается в течение года.

Если убрать из виду отдельные счета, и вообще все конкретное, что мы делали на картах с деньгами, очевидно одно: суммарно за год мы все равно заплатим 1 200 000 &#8381;. Однако оценка, что мы каждый месяц делаем выплату не более 100 000 &#8381; выглядит куда более информативно, чем то, что в худшем случае мы потратим 1 200 000 &#8381;.

Амортизационный анализ как раз и основан на похожих идеях. Он дает возможность вычислять и доказывать более гибкие оценки на суммарную сложность цепочки операций.

## Вычисление средней сложности

Везде далее, в качестве примера, будет изучена следующая ситуация. Рассмотрим пустой вектор целых чисел `std::vector<int>`, над которым последовательно выполнили цепочку из $m$ операций `push_back(x)`. 

Известно, что некоторые из этих операций требуют реаллокации: создание нового участка памяти вдвое большего размера, и последовательное копирование элементов старого вектора в новый. Потому в худшем случае вставка в конец вектора оценивается как $\mathcal{O}(n)$. Представьте, если бы вам в документации заявили такую сложность. 

Совершенно ясно, что большая часть наших `push_back(x)` из цепочки не делают никаких реаллокаций: они просто записывают элемент в память. Быть может, будет уместна средняя оценка на время работы?

<div class="alert alert-definition">
  <img class="alert-icon" src="/assets/images/icons/study.png" alt="icon"><div class="alert-name">Определение</div>
Пусть над некоторой структурой $\mathcal{D}$ совершили цепочку из $m$ операций $a_i$. Пусть временная сложность в худшем случае каждой операции $a_i$ составила $c_i$. Тогда <em>средняя сложность по цепочке</em> вычисляется как 

$$
c_{\text{avg}} = \dfrac{\displaystyle\sum_{i=1}^m c_i}{m}.
$$
<a name="def-average-complexity"></a>
</div>

Важно отметить, что средняя сложность одна и вычисляется для совершенно конкретной цепочки операций.

В нашем случае $a_i=$`push_back`. Понятно, что реаллокаций будут требовать только те вызовы операции `push_back`, при которых количество элементов вектора представляет степень двойки, а это ровно те $a_i$, для которых $i-1$ является степеню двойки. Получаем 

$$
c_i = 1+ \begin{cases}
   i-1,&\text{если } i-1 \text{ степень двойки, }\\
   0, &\text{иначе.}
 \end{cases}
 $$

Следуя недавно сформулированному определению, вычислим среднюю сложность:

$$
c_{\text{avg}} = \dfrac{\displaystyle\sum_{i=1}^m c_i}{m} = 
\dfrac{\displaystyle\sum_{i=1}^m 1 + \sum_{j=1}^{\lfloor \log_2{(m-1)} \rfloor} 2^j}{m} = \dfrac{m+\mathcal{O}(2^{\lfloor\log_2{(m-1)}\rfloor})}{m} =  \dfrac{\mathcal{O}(m)}{m} = \mathcal{O}(1).
$$

Второе равенство следует из верности оценки сверху возрастающей геометрической прогрессии через последний самый большой член.

Таким образом заключаем, что средняя сложность по цепочке `push_back` составляет $\mathcal{O}(1)$, что гораздо более верно оценивает эту операцию в целом. Иными словами, обычно `push_back` очень быстрая операция.


